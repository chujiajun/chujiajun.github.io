---
layout:     post
title:      从0开始的爬虫生活
subtitle:   利用python的scrapy框架编写自己的爬虫程序
date:       2020-04-04
author:     iridescent
header-img: img/post-web.jpg
catalog: true
tags:
    - 爬虫
	- scrapy
	- python
---

# 从0开始的爬虫生活

### 一、环境配置

1. **安装python**

   下载地址：https://www.python.org/downloads/windows
   
   建议python3，我直接选的最新版3.8.2

2. **安装支持库**

   直接pip install “库名”将自动安装对应python版本的支持库
   
   若下载速度过慢导致失败，可手动下载.whl安装包，注意要与python版本对应
   
   下载地址： https://www.lfd.uci.edu/~gohlke/pythonlibs/

   **lxml，用于解析 XML 和 HTML 文档**

   ```
   pip install lxml
   ```

   **cryptography，python中重要的密码学组件**

   ```
   pip install cryptography
   ```

   **twisted，很优秀的网络操作引擎，用于下载器**

   ```
   pip install twisted 
   ```

   **scrapy，目前最好用的爬虫框架，写爬虫的主体**

   ```
   pip install scrapy
   ```



### 二、编写爬虫

1. **创建项目**

   ```bash
   scrapy startproject <项目名称>
   例如：scrapy startproject test_610
   ```

   在当前目录生成项目如下：
   
![image-20200403190044058](C:\Users\10052\AppData\Roaming\Typora\typora-user-images\image-20200403190044058.png)
   
2. **初始化爬虫**

   ```bash
   scrapy genspider <爬虫名称> <start_urls>
   例如：scrapy genspider news sina.com.cn
   ```

   在spiders目录下自动生成一个名为news的文件，后面的地址是要爬取网页的起始地址，文件内容如下：
   
![image-20200403190613083](C:\Users\10052\AppData\Roaming\Typora\typora-user-images\image-20200403190613083.png)
   
3. **编写代码**

   start_urls 内容可以改成要爬取的网页
   
   代码主要写在def parse函数内
   
   代码就没啥可说的了8（瞎写就完事了/滑稽）

4. **运行**

   ```bash
   scrapy crawl <爬虫名称>
   例如：scrapy crawl news
   ```



### 三、自动导入mysql数据库

1. **在items.py中定义item类，内容与mysql数据表项对应**

   ```python
   class Test610Item(scrapy.Item):
       # define the fields for your item here like:
       # name = scrapy.Field()
       categoryId = scrapy.Field()
       title = scrapy.Field()
       abstrs = scrapy.Field()
       tags = scrapy.Field()
       photo = scrapy.Field()
       author = scrapy.Field()
       content = scrapy.Field()
       viewNumber = scrapy.Field()
       commentNumber = scrapy.Field()
       createTime = scrapy.Field()
   
       pass
   ```

2. **在爬虫中创建类对象item，将页面数据解析出来，封装成对象item，yield提交**

   ```python
   def parse(self, response):
   
           item = NewsQqItem()
           
           #解析网页内容省略，此处省略10000字
           #最后封装成item，通过yield提交
           
           item['categoryId'] = 5
           item['title'] = title
           item['abstrs'] = abstrs
           item['tags'] = keyword
           item['photo'] = pic
           item['author'] = 'news'
           item['content'] = content
           item['viewNumber'] = 0
           item['commentNumber'] = 0
           item['createTime'] = time.strftime('%Y-%m-%d %H:%M:%S',time.localtime(time.time()))         
                   
           yield item
   ```

3. **pipelines.py中定义一个类，操作数据库**

   ```python
   #主要修改sql和params两行即可
   
   import MySQLdb
   import MySQLdb.cursors
   from twisted.enterprise import adbapi
   
   class Test610Pipeline(object):  #类名直接用自己项目生成的
       '''保存到数据库中对应的class
          1、在settings.py文件中配置
          2、在自己实现的爬虫类中yield item,会自动执行'''
   
       def __init__(self, dbpool):
           self.dbpool = dbpool
   
       @classmethod
       def from_settings(cls, settings):
           '''1、@classmethod声明一个类方法，而对于平常我们见到的叫做实例方法。
              2、类方法的第一个参数cls（class的缩写，指这个类本身），而实例方法的第一个参数是self，表示该类的一个实例
              3、可以通过类来调用，就像C.f()，相当于java中的静态方法'''
           #读取settings中配置的数据库参数
           dbparams = dict(
               host=settings['MYSQL_HOST'],  
               db=settings['MYSQL_DBNAME'],
               user=settings['MYSQL_USER'],
               passwd=settings['MYSQL_PASSWD'],
               charset='utf8',  # 编码要加上，否则可能出现中文乱码问题
               cursorclass=MySQLdb.cursors.DictCursor,
               use_unicode=False,
           )
           dbpool = adbapi.ConnectionPool('MySQLdb', **dbparams)  # **表示将字典扩展为关键字参数,相当于host=xxx,db=yyy....
           return cls(dbpool)  # 相当于dbpool付给了这个类，self中可以得到
   
       # pipeline默认调用
       def process_item(self, item, spider):
           query = self.dbpool.runInteraction(self._conditional_insert, item)  # 调用插入的方法
           query.addErrback(self._handle_error, item, spider)  # 调用异常处理方法
           return item
   
       # 写入数据库中
       # SQL语句在这里
       def _conditional_insert(self, tx, item):
           sql = "insert into news(categoryId,title,abstrs,tags,photo,author,content,viewNumber,commentNumber,createTime) values(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s)"
           params = (item['categoryId'], item['title'], item['abstrs'], item['tags'], item['photo'], item['author'], item['content'], item['viewNumber'], item['commentNumber'], item['createTime'])
           tx.execute(sql, params)
   
       # 错误处理方法
       def _handle_error(self, failue, item, spider):
           print(failue)
   ```

4. **settings.py中指定数据库操作的类，启用pipelines组件**

   ```python
   ITEM_PIPELINES = {
       'text_610.pipelines.Test610Pipeline': 300, } #保存到mysql数据库
   
   #Mysql数据库的配置信息
   MYSQL_HOST = '127.0.0.1'		#数据库地址
   MYSQL_DBNAME = 'test_610'       #数据库名字
   MYSQL_USER = 'root'             #数据库账号
   MYSQL_PASSWD = '123456'         #数据库密码
   
   MYSQL_PORT = 3306         #数据库端口，在dbhelper中使用
   ```

5. **settings.py中其他设置，比如伪装浏览器请求、延迟抓取等**

   ```python
   USER_AGENT = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3100.0 Safari/537.36'
   ROBOTSTXT_OBEY = False   #不遵守robots协议
   DOWNLOAD_DELAY = 0.25	 #延迟0.25s
   ```

6. **重新运行即可**

   ```bash
   scrapy crawl <爬虫名称>
   ```

   

### 四、同时执行多个爬虫

1. **在spiders同级目录下新建文件夹commands**

   ![image-20200404164118768](C:\Users\10052\AppData\Roaming\Typora\typora-user-images\image-20200404164118768.png)

2. **commands文件夹目录下新建crawlall.py文件**

   ```python
   import os
   from scrapy.commands import ScrapyCommand
   from scrapy.utils.conf import arglist_to_dict
   from scrapy.utils.python import without_none_values
   from scrapy.exceptions import UsageError
    
   class Command(ScrapyCommand):
       requires_project = True
       def syntax(self):
           return "[options] <spider>"
       def short_desc(self):
           return "Run all spider"
       def add_options(self, parser):
           ScrapyCommand.add_options(self, parser)
           parser.add_option("-a", dest="spargs", action="append", default=[], metavar="NAME=VALUE",
                             help="set spider argument (may be repeated)")
           parser.add_option("-o", "--output", metavar="FILE",
                             help="dump scraped items into FILE (use - for stdout)")
           parser.add_option("-t", "--output-format", metavar="FORMAT",
                             help="format to use for dumping items with -o")
    
       def process_options(self, args, opts):
           ScrapyCommand.process_options(self, args, opts)
           try:
               opts.spargs = arglist_to_dict(opts.spargs)
           except ValueError:
               raise UsageError("Invalid -a value, use -a NAME=VALUE", print_help=False)
           if opts.output:
               if opts.output == '-':
                   self.settings.set('FEED_URI', 'stdout:', priority='cmdline')
               else:
                   self.settings.set('FEED_URI', opts.output, priority='cmdline')
               feed_exporters = without_none_values(
                   self.settings.getwithbase('FEED_EXPORTERS'))
               valid_output_formats = feed_exporters.keys()
               if not opts.output_format:
                   opts.output_format = os.path.splitext(opts.output)[1].replace(".", "")
               if opts.output_format not in valid_output_formats:
                   raise UsageError("Unrecognized output format '%s', set one"
                                    " using the '-t' switch or as a file extension"
                                    " from the supported list %s" % (opts.output_format,tuple(valid_output_formats)))
               self.settings.set('FEED_FORMAT', opts.output_format, priority='cmdline')
    
       def run(self, args, opts):
           #获取爬虫列表
           spd_loader_list=self.crawler_process.spider_loader.list()
           print(spd_loader_list)
           #遍历各爬虫
           for spname in spd_loader_list or args:
               self.crawler_process.crawl(spname, **opts.spargs)
               print (spname + ' spider is running')
           self.crawler_process.start()
   ```

3. **目录下新建空白\_init\_.py文件**

4. **settings.py中加一句**

   ```python
   COMMANDS_MODULE = 'test_610.commands'
   ```

5. **执行scrapy -h，查看可用指令是否有crawlall**

   ![image-20200404163845331](C:\Users\10052\AppData\Roaming\Typora\typora-user-images\image-20200404163845331.png)

6. **执行scrapy crawlall，即可同时运行全部爬虫**

   ```bash
   scrapy crawlall
   ```



### 五、定时执行

方法很多，随便说两种

可以直接在代码中通过time.sleep()实现

也可以把执行语句编写成bat批处理文件，然后电脑直接对bat文件设定计划任务



### 六、开发过程中的BUG日记

1. **VS Code提示：The environment variable 'Path' seems to have some paths containing the '"' character.**

   解决方法：删掉环境变量中的分号

2. **[Failure instance: Traceback: <class 'MySQLdb._exceptions.OperationalError'>: (1366, "Incorrect string value: '\\xF0\\x9F\\x99\\x8F</...' for column 'content' at row 1")**

   原因：文本有emoji符号，是4个字节，而Mysql的utf-8编码最多3个字节，所以数据插不进去

   解决方法： 

   1. 在mysql的安装目录下找到my.ini，作如下修改： 

      ```
      [mysqld]
      
      character-set-server=utf8mb4
      
      [mysql]
      
      default-character-set=utf8mb4
      ```

      修改后重启Mysql

   2. 将已经建好的表也转换成utf8mb4

      ```sql
      命令：alter table TABLE_NAME convert to character set utf8mb4 collate utf8mb4_bin; （将TABLE_NAME替换成你的表名） 
      ```

   3. pipelines.py中连接数据库，编码方式改成：

      ```python
      charset='utf8mb4'
      ```

3. **web项目中使用了easyui组件，部署到tomcat服务器上中文显示变成乱码** 

   1. 修改Tomcat 9.0\conf\server.xml，添加

      ```
      URIEncoding="UTF-8"
      ```

      ![](C:\Users\10052\AppData\Roaming\Typora\typora-user-images\image-20200406170944490.png)

   2. 修改Tomcat 9.0\bin\catalina.bat，在第二行添加

      ```
      set "JAVA_OPTS=%JAVA_OPTS% %JSSE_OPTS%  -Dfile.encoding=UTF-8"
      ```

      ![img](https://img-blog.csdnimg.cn/20190115145657102.png)

   3. 重启tomcat，清除浏览器缓存

4. **mysql数据库表中，删掉某些记录，怎么让新加的数据id连续（克服强迫症）**

   ```sql
   SET @i=0;
   UPDATE tablename SET id=(@i:=@i+1);
   ALTER TABLE tablename AUTO_INCREMENT=0  <!--这里的0设置成想接着的id-->
   ```